# -*- coding: utf-8 -*-
"""03_다중회귀.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x2bVFuf3R07-p84z1HIwSUlAl0hRtfnP

# 다중회귀 - 보스톤 주택 가격
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_boston

import warnings
warnings.filterwarnings('ignore')

boston = load_boston()

df = pd.DataFrame(boston.data, columns = boston.feature_names)
df['PRICE'] = boston.target
df.head(3)

"""### 산점도"""

sns.regplot(x = "CRIM", y = 'PRICE', data  =df);

_, ax = plt.subplots(3, 4, figsize = (16, 12))

for i, feature in enumerate(df.columns[1:-1]):
    row, col = i // 4, i % 4
    sns.regplot(x = feature, y = 'PRICE', data = df,ax = ax[row, col]);

"""#### feature별 결정계수(r squared)"""

from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.get_params()

for feature in df.columns[:-1]:
    lr = LinearRegression()
    X = df[[feature]]
    lr.fit(X, boston.target)
    score =lr.score(X, boston.target)
    print(f'{feature}\t:{round(score,4)}')

"""## 다중선형회귀
- 13개의 독립변수를 모두 쓰고 후진제거법
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    boston.data, boston.target, test_size= 0.1, random_state=2022
)

lr = LinearRegression()
lr.fit(X_train, y_train)

# 수정된 결정계수
lr.score(X_train, y_train)

lr.coef_  # 계수(가중치) - weight

lr.intercept_ # 오차항, bias

"""- 테스트데이터에 적용"""

X_test[0], y_test[0]

np.dot(lr.coef_, X_test[0]) + lr.intercept_

lr.predict(X_test[0].reshape(1, -1))

for i in range(10):
    pred1 = np.dot(lr.coef_, X_test[i]) + lr.intercept_
    pred2 = lr.predict(X_test[i].reshape(1,-1))
    print(f'실제값 : {y_test[i]}\t직접계산 : {pred1:.4f}\t예측값 : {pred2[0]:.4f}')

"""## 회귀는 꼭 선형 회귀를 써야하는가?
- 선형회귀
- 결정트리
- SVM
- random forst
- XGBoost
"""

from sklearn.metrics import r2_score, mean_squared_error

"""### 1.선형회귀"""

pred_lr = lr.predict(X_test)
r2_lr = r2_score(y_test, pred_lr)
mse_lr = mean_squared_error(y_test, pred_lr)

"""### 2.Decision Tree

"""

from sklearn.tree import DecisionTreeRegressor
dtr = DecisionTreeRegressor(random_state = 2022)
dtr.fit(X_train, y_train)
pred_dtr = dtr.predict(X_test)
r2_dtr= r2_score(y_test, pred_dtr)
mse_dt = mean_squared_error(y_test, pred_dtr)

"""### 3.SVM"""

from sklearn.svm import SVR

svr = SVR()
svr.fit(X_train, y_train)
pred_svr = svr.predict(X_test)
r2_svr= r2_score(y_test, pred_svr)
mse_svr = mean_squared_error(y_test, pred_svr)

"""### 4.Random_Forest"""

from sklearn.ensemble import RandomForestRegressor

rfr = RandomForestRegressor(random_state = 2022)
rfr.fit(X_train, y_train)
pred_rfr = rfr.predict(X_test)
r2_rfr= r2_score(y_test, pred_rfr)
mse_rfr = mean_squared_error(y_test, pred_rfr)

"""### 5.XGBoost"""

from xgboost import XGBRegressor

xgr = XGBRegressor()
xgr.fit(X_train, y_train)
pred_xgr = xgr.predict(X_test)
r2_xgr= r2_score(y_test, pred_xgr)
mse_xgr = mean_squared_error(y_test, pred_xgr)

"""#### 비교"""

print(f'선형회귀\tR2 : {r2_lr:.4f}\tMSE : {mse_lr:.4f}')
print(f'결정트리\tR2 : {r2_dtr:.4f}\tMSE : {mse_dt:.4f}')
print(f'SVM\t\tR2 : {r2_svr:.4f}\tMSE : {mse_svr:.4f}')
print(f'random forst\tR2 : {r2_rfr:.4f}\tMSE : {mse_rfr:.4f}')
print(f'XGBoost\t\tR2 : {r2_xgr:.4f}\tMSE : {mse_xgr:.4f}')

df2 = pd.DataFrame({
    'y_test' : y_test, 'LR':pred_lr, 'DT' : pred_dtr, 'SVM' : pred_svr,
    'RF' : pred_rfr, 'XGB' : pred_xgr
})

df2.head()

